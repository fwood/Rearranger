% !TEX root = main.tex
\section{Discussion}
\label{sec:discussion}

Our Bayesian approach to PDIA inference can be interpreted as a stochastic search procedure for PDFA structure learning where the number of states is unknown.  In Section~\ref{sec:theory} we presented evidence that PDFA samples from our PDIA inference algorithm have the same characteristics as the true generative process\comment{ and this suggests that the MAP sample may often be the exact, or equivalent to the exact after transformation, generative PDFA}.  This in and of itself may be of interest to the PDFA induction community.  

We ourselves are more interested in establishing new ways to produce smoothed predictive conditional distributions.  Inference in the PDIA presents a completely new approach to smoothing, smoothing by averaging over PDFA model structure rather than hierarchically smoothing related emission distribution estimates.   Our PDIA approach gives us an attractive ability to trade-off between model simplicity in terms of number of states, computational complexity in terms of asymptotic cost of prediction, and predictive perplexity.  While our PDIA approach may not yet outperform the best smoothing Markov model approaches in terms of predictive perplexity alone, it does outperform them in terms of model complexity required to achieve the same predictive perplexity, and outperforms HMMs in terms of asymptotic time complexity of prediction.  This suggests that a future combination of smoothing over model structure {\em and} smoothing over emission distributions could produce excellent results.  PDIA inference gives researchers another tool to choose from when building models.  If very fast prediction is desirable and the predictive perplexity difference between the PDIA and, for instance, the most competitive $n$-gram is insignificant from an application perspective, then doing finite sample inference in the PDIA offers a significant computational advantage in terms of memory.

We indeed believe the most promising approach to improving PDIA predictive performance is to construct a smoothing hierarchy over the state specific emission distributions, as is done in the smoothing $n$-gram models.  For an $n$-gram, where every state corresponds to a suffix of the sequence, the predictive distributions for a suffix is smoothed by the predictive distribution for a shorter suffix, for which there are more observations.  This makes it possible to increase the size of the model indefinitely without generalization performance suffering \cite{Wood2009}.  In the PDIA, by contrast, the predictive probabilities for states are not tied together.  Since states of the PDIA are not uniquely identified by suffixes, it is no longer clear what the natural smoothing hierarchy is.  It is somewhat surprising that PDIA learning works nearly as well as $n$-gram modeling even without a smoothing hierarchy for its emission distributions.  Imposing a hierarchical smoothing of the PDIA emission distributions remains an open problem. \comment{However, if such a hierarchy does exist, using it as the generative model for the emission distributions $\boldsymbol\pi$ should lead to superior performance, even surpassing $n$-gram models.}

%Another path for further research is to improve MCMC mixing.  Two PDFAs that differ by only a single transition $\delta_{ij}$ may generate very different probabilistic languages.  A more natural measure of the distance between PDFAs is given by the KL divergence between the languages they generate \cite{Carrasco1994}.  An MCMC sampler that proposes PDFAs that are similar in the KL divergence sense may mix more efficiently than one that randomly reassigns $\delta_{ij}$.  Similar to existing PDFA induction algorithms \cite{Shalizi2004,Thollard2000}, merging and splitting states may be an effective way to achieve this.

\comment{Much of the original work on finite automata was motivated by early studies of neural nets \cite{Hopcroft1979}. It was shown that the earliest neural nets \cite{McCulloch1943} were equivalent to finite automata \cite{Kleene1956}, and much of the field shifted to automata theory.  Now there is renewed interest in stochastic neural networks, for machine learning \cite{Hinton2006}, data analysis of biological neurons \cite{Pillow2008} and other applications.  Connections to probabilistic automata have not been well explored. \comment{ and models of biological computation \cite{Pouget? Lengyel? Fiser?}.   The expressive power of these models is not well characterized.  The theory of probabilistic automata may provide a framework for analyzing these models and perhaps even a route to generalization. } Our results demonstrate that finding novel approaches to learning simple models can be as fruitful as tackling more complex models, and may even open a route to new models of computation.}
