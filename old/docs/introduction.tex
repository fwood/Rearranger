% !TEX root = main.tex
\section{Introduction}
\comment{All learning algorithms must strike a compromise between computational efficiency, model complexity, and generalization performance.  One extreme in terms of searching for minimally complex models is to search for the shortest computer program that can reproduce the observed data exactly. Unfortunately performing this search is well known to be computationally challenging, and, even if found, any single resulting program is not likely to generalize well given only a small number of observations\comment{generated by a complex mechanism}.   A mixture of such programs weighted by length, on the other hand, is the universal prior defined by Solomonoff \cite{Solomonoff1964,Solomonoff1978} and has good theoretical generalization properties.  Of course, given that searching for a single such program is costly, learning a mixture of programs becomes practically impossible.  

The focus of this paper is a similar in spirit but restricts the search in ways that render our approach computable.  Instead of learning a mixture of programs that can reproduce the data exactly, we, in effect, learn a mixture of probabilistic deterministic finite automata (PDFA), each of which can reproduce the data exactly.   We do this using a novel Bayesian framework for PDFA learning.  In this framework we first specify a prior over the parameters of a single large PDFA that encourages state reuse.  The inductive bias introduced by the prior can be interpreted as a kind of soft constraint that limits the number of states actually used by the PDFA to generate the observed data (its ``complexity'').   Being a Bayesian approach, we retain and average over our uncertainty about both the cardinality of states in the automata used to generate the data, the links between those states, and the emission distributions.  The posterior distribution over PDFA parameter settings can be interpreted as an infinite mixture over PDFAs.  A set of samples drawn from this distribution via, for instance, Markov chain Monte Carlo (MCMC) can be interpreted as a finite sample approximation to this infinite mixture, where again each sample is a PDFA of potentially varying complexity.  When performing inference we average over this posterior distribution, yielding a novel approach to smoothing over PDFAs, a technique known to produce good generalization results  \cite{pdfa_smoothing_approaches}.} % with a different number of states (only those that are used to generate the data matter), different emission distributions, and so forth.  
%The expressivity of a single PDFA with, to be redundant but necessarily pedantic, a finite number of states, is restrictive.  A mixture of PDFAs is less restrictive, in fact, it is know

The focus of this paper is a novel Bayesian framework for learning with probabilistic deterministic finite automata (PDFA) \cite{Rabin1963}.  A PDFA is a generative model for sequential data (PDFAs are reviewed in  Section \ref{sec:PDFA}).  Intuitively a PDFA is similar to a hidden Markov model (HMM) \cite{Rabiner1989} in that it consists of a set of states, each of which when visited emits a symbol according to an emission probability distribution.  It differs from an HMM in how state-to-state transitions occur; transitions are deterministic in a PDFA and nondeterministic in an HMM.  

In our framework for learning with PDFAs we specify a prior over the parameters of a single large PDFA that encourages state reuse.  The inductive bias introduced by the PDFA prior provides a soft constraint on the number of states used to generate the data.  We take the limit as the number of states becomes infinite, yielding a model we call the probabilistic deterministic infinite automata (PDIA).  

Given a finite training sequence, the PDIA posterior distribution is an infinite mixture of PDFAs.  Samples from this distribution form a finite sample approximation to this infinite mixture, and can be drawn via Markov chain Monte Carlo (MCMC) \cite{Gelman1995}.  Using such a mixture we can average over our uncertainty about the model parameters (including state cardinality) in a Bayesian way during prediction and other inference tasks.  We find that averaging over a finite number of PDFAs trained on naturalistic data leads to better predictive performance than using a single ``best'' PDFA.  

We chose to investigate learning with PDFAs because they are intermediate in expressive power between HMMs and finite-order Markov models, and thus strike a good balance between generalization performance and computational efficiency.  A single PDFA is known to have relatively limited expressivity.  We argue in \ref{sec:theory} that a finite mixture of PDFAs has greater expressivity than that of a single PDFA but is not as expressive as a probabilistic nondeterministic finite automata (PNFA)\footnote{PNFAs with no final probability are equivalent to hidden Markov models \cite{Dupont2005} \label{fn:pnfa}} .  A PDIA is clearly highly expressive; an infinite mixture over the same is even more so.  Even though ours is a Bayesian approach to PDIA learning, in practice we only ever deal with a finite approximation to the full posterior and thus limit our discussion to finite mixtures of PDFAs.

While model expressivity is a concern, computational considerations often dominate model choice.  We show that prediction in a trained mixture of PDFAs can have lower asymptotic cost than forward prediction in the PNFA/HMM\comment{\footref{fn:pnfa}} class of models.  We also present evidence that averaging over PDFAs gives predictive performance superior to HMMs trained with standard methods on naturalistic data.  We find that PDIA predictive performance is competitive with that of fixed-order, smoothed Markov models with the same number of states.  While sequence learning approaches such as the HMM and smoothed Markov models are well known and now highly optimized, our PDIA approach to learning is novel and is amenable to future improvement.  

Section \ref{sec:PDFA} reviews PDFAs, Section \ref{sec:BPDFAs} introduces Bayesian PDFA inference, Section \ref{sec:results} presents experimental results on DNA and natural language, and Section \ref{sec:theory} discusses related work on PDFA induction and the theoretical expressive power of mixtures of PDFAs.
In Section \ref{sec:discussion} we discuss ways in which PDIA predictive performance might be improved in future research.

\comment{
 We wish to learn a mixture of probabilistic deterministic finite automata any of which could have generated the observed data.  of Bayesian learning of a probabilistic deterministic infinite automata (PDIA).  A probabilistic deterministic automata with an infinite number of states is a  a class of models that includes variable and fixed order Markov models as a special case, as well as simpler models.  

%At a high level, a PDFA can be thought of as a hidden Markov model for which, given an observed sequence, there is only one possible path through the hidden states.  A more formal definition follows in section 2.
We motivate our choice of model class several ways.  First, it can be shown by a simple argument that, given an infinite and stationary sequence, the minimal sufficient statistics of the past for predicting the future form a PDFA \cite{Crutchfield1999}.  That is, given two pasts that map to the same statistic, the concatenation of those pasts with the same symbol will map to the same statistic.  This is not the case in general hidden Markov models, where many transitions are possible from a hidden state, though observed data may change the posterior probability of those transitions.  Existing algorithms for learning these statistics use tests that have asymptotic guarantees but may not work well for reasonable amounts of data \cite{Shalizi2004}.

Another argument is that we are trying to generalize the class of variable-length Markov models, which have had great empirical success in sequence prediction.

N-gram models have had great empirical success in sequence prediction.  However, there is no clear way to trade off model complexity with prediction accuracy.  In extensions to n-gram models which can learn from arbitrarily long contexts, the model complexity will grow without bound, even for trivially simple sequences such as repetitions of a few characters.  Bounded memory models perform well in practice, but we would much prefer to learn a model that is as small as possible for very simple data, while growing large for more complex data.  A natural class of models to explore is probabalistic deterministic finite automata (PDFA), which contains n-grams as a special case, as well as simpler models.  We define a prior over PDFAs with a finite number of states and describe a Metropolis-Hastings algorithm to generate samples from the posterior.  We then generalize to the case of PDFAs with a (potentially) infinite number of states and show that the generative model is a type of Hierarchical Dirichlet Process (HDP).  We then (I hope!) describe a state splitting/merging algorithm for posterior inference that mixes more efficiently than the original Metropolis-Hastings algorithm, and show that it defines a natural hierarchy of states for smoothing (fingers crossed...)  The set of strings produced by a PDFA constitutes a probabilistic regular language, thus our inference procedure can also be viewed as a non-greedy algorithm for regular grammar induction.

Finally, }