\section{Related Work}

Despite a wealth of research on both the theory and practice of learning PDFAs, the work presented here is, to our knowledge, the first algorithm to generate samples from a posterior over automata rather than returning a deterministic estimate.  Prior work has focused on greedy algorithms, which work by either merging or splitting elements of $Q$ according to some statistical test.  Theoretical work has shown that PDFAs are both identifiable in the limit and, with a few restrictions on the model class\footnote{A polynomial number of states in the true automata, a minimum divergence between the distribution over strings that follow two states, and a polynomial bound on the probability of generating strings above a certain length}, are also PAC-learnable\footnote{A model class is said to be {\em PAC-learnable} if there is an algorithm that will return, in time polynomial in $\frac{1}{\delta}$,$\frac{1}{\epsilon}$ and $|D|$, an estimate within an accuracy $\epsilon$ of the true model from $|D|$ examples with probability $1-\delta$.} using KL divergence between automata as a measure of accuracy.

\subsection{State Merging Algorithms}
A variety of algorithms work by starting with the trivial automata built from the prefix tree of the data, and generalizes by merging states that pass a similarity test.  Merging two states is not trivial: if $\delta(q_1,s_j) \ne \delta(q_2,s_j)$, then merging $q_1$ and $q_2$ will produce a state with nondeterministic transitions.  This is avoided by recursively merging the states $\delta_{1j}$ and $\delta_{2j}$ until the resulting automata is deterministic.  The result is a {\em quotient automata} of the original.  One of the earliest algorithms to use this method is ALERGIA \cite{Carrasco1994}, which uses a test based on the Hoeffding bound to decide whether to merge states, and was proven to converge to the true automata in the limit of infinite data.

Later algorithms have mostly focused on improving the state merging test.  MDI uses a test based on the KL divergence between automata, and penalizes automata with many states.  Thus it can be interpreted as a greedy maximum posterior estimator with a minimum description length prior.  Empirically, it has better predictive performance than ALERGIA on natural language data.

\cite{Clark2004} presented a state-merging algorithm for cyclic PDFAs that is PAC-learnable given very few restrictions on the model class.  While the emphasis was on theoretical rather than empirical performance, it was also shown to work well from small data.

\subsection{State Splitting Algorithms}

State splitting algorithms, by contrast, start with the most general single-state automata and become more selective by adding more states.  \cite{Ron1996} learned a variable-order Markov model using a state-splitting algorithm, where a state corresponding to a string suffix was split into states corresponding to longer suffixes according to some test (*hm...all this "according to some test" is getting redundant.  Might want to rephrase this*).  Splitting might occasionally produce nondeterministic transitions.  For example, after splitting the context $01$ into $001$ and $101$, the context $0$ and symbol $1$ might transition to either one, unless the context $0$ is also split into $10$ and $00$, but these probabilistic suffix trees could be mapped onto PDFAs after learning.

CSSR \cite{Shalizi2004} took a similar approach, but with a model class that contains all PDFAs.  Their philosophical motivation, similar to ours, is to learn the minimal sufficient statistics for predicting the future given the past.  Given a stationary sequence with infinitely long past and future, those statistics form a PDFA, which they call a {\em causal state machine}.  Each state is a set of suffixes, rather than a single suffix, which means the model class includes all PDFAs.  If the predictive distribution for a suffix passes a Kolmogorov-Smirnov test, that state is split in two and suffixes in the original state are divided.  Nondeterministic transitions are removed recursively by backing up to the states preceding a split state and splitting in the natural way, much like the reverse of how states are merged when forming quotient automata.
