% !TEX root = main.tex
\begin{abstract}
%A common desire is to learn minimally complex, maximally predictive models for data.  
We propose a novel Bayesian nonparametric approach to learning with probabilistic deterministic finite automata (PDFA).  We define and develop a sampler for a PDFA with an infinite number of states which we call the probabilistic deterministic infinite automata (PDIA).   Posterior predictive inference in this model, given a finite training sequence,  can be interpreted as averaging over multiple PDFAs of varying structure, where each PDFA is biased towards having few states.  We suggest that our method for averaging over PDFAs is a novel approach to predictive distribution smoothing.    We test PDIA inference both on PDFA structure learning and on both natural language and DNA data prediction tasks.  The results suggest that the PDIA presents an attractive compromise between the computational cost of hidden Markov models and the storage requirements of hierarchically smoothed Markov models. %, albeit at some cost in terms of predictive performance.    %We introduce a nonparametric prior distribution over a PDIA and a Markov chain Monte Carlo sampling procedure for posterior estimation.  We show that the posterior distribution over the PDIA consists of an infinite mixture of PDFA and as a result the model has the same expressive power as a \fix{hidden Markov model, while retaining properties of a variable or fixed order Markov model that are attractive for predicting sequences}.  We demonstrate the performance of our approach on natural language and DNA, achieving results compatible with state of the art models.  \fix{We argue that this approach opens the door to new approaches to new ways of computing.}{Say something about new way of computing}
\end{abstract}