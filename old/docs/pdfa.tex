% !TEX root = main.tex
\section{Probabilistic Deterministic Finite Automata}
\label{sec:PDFA}

A PDFA is formally defined as a 5-tuple $M = (Q,\Sigma,\delta,\pi,\state_0)$, where $Q$ is a finite set of states, $\Sigma$ is a finite alphabet of observable symbols, $\delta\,:\,Q\times\Sigma\rightarrow Q$ is the transition function from a state/symbol pair to the next state, $\pi\,:\,Q\times\Sigma\rightarrow[0,1]$ is the probability of the next symbol given a state and $\state_0$ is the initial state.\footnote{In general $\state_0$ may be replaced by a distribution over initial states.  \comment{We are interested in inference when the path is known exactly, and so restrict ourselves to the case of a single initial state.}}  Throughout this paper we will use $i$ to index elements of $Q$, $j$ to index elements of $\Sigma$, and $t$ to index elements of an observed string.  For example, $\delta_{ij}$ is shorthand for $\delta(\state_i,\symb_j)$, where $\state_i \in Q$ and $\symb_j \in \Sigma$.

Given a state $\state_i$, the probability that the next symbol takes the value $\symb_j$ is given by $\pi(\state_i,\symb_j)$.  We use the shorthand $\bpi_{q_i}$ for the state-specific discrete distribution over symbols for state $\state_i$.  We can also write $\sigma|\state_i \sim \bpi_{q_i}$ where $\sigma$ is a random variable that takes values in $\Sigma$.  Given a state $\state_i$ and a symbol $\symb_j$, however, the next state $\state_{i'}$ is {\it deterministic}: $\state_{i'} = \delta(\state_i,\symb_j).$   Generating from a PDFA involves first generating a symbol stochastically given the state the process is in: $x_t|\q_t \sim \bpi_{\q_t}$ where $\q_t \in Q$ is the state at time $t$.  Next, given $\q_t$ and $x_t$ transitioning deterministically to the next state: $\q_{t+1} = \delta(\q_t,x_t)$.  This is the reason for the confusing ``probabilistic deterministic'' name for these models.  Turning this around, given data, $q_0$, and $\delta$, there is no uncertainty about the path through the states.  This is a primary source of computational savings relative to HMMs.

PDFAs are more general than $n$th-order Markov models (i.e. $m$-gram models, $m=n+1$), but less expressive than hidden Markov models (HMMs)\cite{Dupont2005}.  For the case of $n$th-order Markov models, we can construct a PDFA with one state per suffix $x_1 x_2 \ldots x_n$.  Given a state and a symbol $x_{n+1}$, the unique next state is the one corresponding to the suffix $x_2 \ldots x_{n+1}$.  Thus $n$th-order Markov models are a subclass of PDFAs with $\mathcal{O}(|\Sigma|^n)$ states.  For an HMM, given data and an initial distribution over states, there is a posterior probability for every path through the state space.  PDFAs are those HMMs for which, given a unique start state, the posterior probability over paths is degenerate at a single path.  As we explain in Section \ref{sec:theory}, mixtures of PDFAs are strictly more expressive than single PDFAs, but still less expressive than PNFAs.